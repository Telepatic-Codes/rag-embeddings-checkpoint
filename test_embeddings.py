# test_embeddings.py
# VersiÃ³n de prueba que no requiere API key
# Te muestra cÃ³mo funcionan los embeddings y la similitud semÃ¡ntica

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

print("ğŸ§  APRENDIENDO SOBRE EMBEDDINGS Y SIMILITUD SEMÃNTICA")
print("=" * 60)
print()

# Simulamos embeddings para diferentes textos
# En la realidad, estos vectores serÃ­an generados por el modelo de OpenAI
print("ğŸ“Š SIMULACIÃ“N DE EMBEDDINGS")
print("Los embeddings son vectores numÃ©ricos que representan el significado del texto")
print()

# Textos de ejemplo
texts = [
    "hola mundo",      # Saludo bÃ¡sico
    "perro feliz",     # Animal con emociÃ³n  
    "limÃ³n verde",     # Fruta con color
    "gato contento",   # Animal con emociÃ³n (similar a perro feliz)
    "casa grande",     # Objeto con tamaÃ±o (diferente a los anteriores)
]

# Simulamos embeddings (vectores de 5 dimensiones para simplificar)
# En la realidad, los embeddings de OpenAI tienen 1536 dimensiones
mock_embeddings = [
    [0.1, 0.2, 0.3, 0.1, 0.1],  # hola mundo
    [0.8, 0.9, 0.1, 0.2, 0.1],  # perro feliz
    [0.1, 0.1, 0.8, 0.9, 0.2],  # limÃ³n verde
    [0.7, 0.8, 0.2, 0.3, 0.1],  # gato contento (similar a perro feliz)
    [0.2, 0.1, 0.1, 0.1, 0.9],  # casa grande (diferente)
]

print("Textos procesados:")
for i, text in enumerate(texts):
    print(f"  {i+1}. '{text}' -> Vector: {mock_embeddings[i]}")
print()

# Calculamos la matriz de similitud
print("ğŸ” CALCULANDO SIMILITUD SEMÃNTICA")
print("La similitud coseno mide quÃ© tan parecidos son dos vectores (0-1)")
print()

sim_matrix = cosine_similarity(np.array(mock_embeddings))

# Mostramos la matriz de forma clara
print("Matriz de similitud (coseno):")
print("Valores cercanos a 1 = muy similares")
print("Valores cercanos a 0 = muy diferentes")
print()

# Encabezado
print("          ", end="")
for text in texts:
    print(f"{text:>12}", end="")
print()

# Filas de la matriz
for i, text in enumerate(texts):
    print(f"{text:>12}", end="")
    for j in range(len(texts)):
        similarity = sim_matrix[i][j]
        print(f"{similarity:>12.3f}", end="")
    print()

print()
print("ğŸ“ˆ ANÃLISIS DE RESULTADOS")
print("=" * 40)

# Encontramos las similitudes mÃ¡s altas (excluyendo la diagonal)
max_similarity = 0
most_similar_pair = None

for i in range(len(texts)):
    for j in range(i+1, len(texts)):  # Solo comparamos pares diferentes
        similarity = sim_matrix[i][j]
        print(f"â€¢ '{texts[i]}' vs '{texts[j]}': {similarity:.3f}")
        
        if similarity > max_similarity:
            max_similarity = similarity
            most_similar_pair = (texts[i], texts[j])

print()
print(f"ğŸ¯ PARES MÃS SIMILARES:")
print(f"   '{most_similar_pair[0]}' y '{most_similar_pair[1]}' ({max_similarity:.3f})")
print()

print("ğŸ’¡ CONCEPTOS CLAVE:")
print("â€¢ Los embeddings capturan el significado, no solo palabras exactas")
print("â€¢ Textos con significado similar tienen embeddings similares")
print("â€¢ La similitud coseno mide el Ã¡ngulo entre vectores")
print("â€¢ Valores cercanos a 1 = muy similares, cercanos a 0 = muy diferentes")
print()

print("ğŸš€ PRÃ“XIMO PASO:")
print("Para usar embeddings reales, necesitas:")
print("1. Una API key de OpenAI")
print("2. Configurar el archivo .env")
print("3. Ejecutar scratch_embeddings.py") 